[
  {
    "objectID": "textProject.html",
    "href": "textProject.html",
    "title": "Project 3",
    "section": "",
    "text": "library(quanteda)\n\nWarning: package 'quanteda' was built under R version 4.3.3\n\n\nWarning in .recacheSubclasses(def@className, def, env): undefined subclass\n\"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\n\nPackage version: 3.3.1\nUnicode version: 15.1\nICU version: 74.1\n\n\nParallel computing: 16 of 16 threads used.\n\n\nSee https://quanteda.io for tutorials and examples.\n\nlibrary(quanteda.textplots)\n\nWarning: package 'quanteda.textplots' was built under R version 4.3.3\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(nycflights13)\nlibrary(ggplot2)\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(httr)\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.3.3"
  },
  {
    "objectID": "textProject.html#the-us-inaugural-corpus",
    "href": "textProject.html#the-us-inaugural-corpus",
    "title": "Project 3",
    "section": "The US Inaugural Corpus",
    "text": "The US Inaugural Corpus\nUS presidential inaugural speeches serve as pivotal moments for setting the national tone, articulating priorities, and establishing a unified camaraderie as the nation embarks into a new era. Beyond their ceremonial significance, these events can be seen as historical snapshots, capturing the promises, aspirations, and challenges of the U.S. during their respective periods. With these ideas in mind, let’s look at trends in the language used and how these speeches have changed over time or by party, and eventually, run sentimental analysis on their vocabulary choices and discover what insights this can bring.\nInterestingly enough, there are nine speeches that don’t have a single named reference of “America” (or “american”, “americans”, etc.) within them.\n\nno_merica &lt;- is.na(str_extract(data_corpus_inaugural, \".merica\"))\n\nsummary(data_corpus_inaugural) |&gt;\n  filter(no_merica)\n\nCorpus consisting of 59 documents, showing 59 documents:\n\n           Text Types Tokens Sentences Year President   FirstName\n 1801-Jefferson   717   1923        41 1801 Jefferson      Thomas\n   1809-Madison   535   1261        21 1809   Madison       James\n     1825-Adams  1003   3147        74 1825     Adams John Quincy\n   1829-Jackson   517   1208        25 1829   Jackson      Andrew\n      1845-Polk  1334   5186       153 1845      Polk  James Knox\n     1869-Grant   485   1229        40 1869     Grant  Ulysses S.\n     1873-Grant   552   1472        43 1873     Grant  Ulysses S.\n 1905-Roosevelt   404   1079        33 1905 Roosevelt    Theodore\n    1913-Wilson   658   1882        68 1913    Wilson     Woodrow\n                 Party\n Democratic-Republican\n Democratic-Republican\n Democratic-Republican\n            Democratic\n                  Whig\n            Republican\n            Republican\n            Republican\n            Democratic\n\n\nMany of these opt to use terms like “the Nation” or “this Republic”, finding substitutes in place of name-dropping their country. However, the most recent of these took place in 1913 with Wilson’s first inaugural address, which could suggest that this practice is outdated in the modern era. Still, the fact that these former presidents were able to make a rallying cry without naming their homeland is impressive, and seems like a difficult task made even more tricky.\n\nter_mentioned &lt;- !is.na(str_extract(data_corpus_inaugural, \"[Tt]error\"))\n\nsummary(data_corpus_inaugural) |&gt;\n  filter(ter_mentioned)\n\nCorpus consisting of 59 documents, showing 59 documents:\n\n           Text Types Tokens Sentences Year President   FirstName      Party\n     1797-Adams   826   2577        37 1797     Adams        John Federalist\n 1933-Roosevelt   743   2057        85 1933 Roosevelt Franklin D. Democratic\n 1941-Roosevelt   526   1519        68 1941 Roosevelt Franklin D. Democratic\n   1961-Kennedy   566   1541        52 1961   Kennedy     John F. Democratic\n    1981-Reagan   902   2780       129 1981    Reagan      Ronald Republican\n   1997-Clinton   773   2436       111 1997   Clinton        Bill Democratic\n     2009-Obama   938   2689       110 2009     Obama      Barack Democratic\n     2017-Trump   582   1660        88 2017     Trump   Donald J. Republican\n     2021-Biden   812   2766       216 2021     Biden   Joseph R. Democratic\n\n\nWe can see that outside of one mention way back in 1797, terror is largely a modern adversity for the US. Especially so in the wake of September 11th, nearly every president in recent memory has mentioned it at least once within their speeches, surprisingly however, Bush did not.\n\nstr_view(str_extract(data_corpus_inaugural[3], \"\\\\..*terror.*\\\\.\"), \"terror\")\n\n[1] │ . If an election is to be determined by a majority of a single vote, and that can be procured by a party through artifice or corruption, the Government may be the choice of a party for its own ends, not of the nation for the national good. If that solitary suffrage can be obtained by foreign nations by flattery or menaces, by fraud or violence, by &lt;terror&gt;, intrigue, or venality, the Government may not be the choice of the American people, but of foreign nations. It may be foreign nations who govern us, and not we, the people, who govern ourselves; and candid men will acknowledge that in such cases choice would have little advantage to boast of over lot or chance.\n\n\nHere we can see Adams’ reference to terror is more in line with the idea of a fearsome government like the “Reign of Terror” that had just occurred in France earlier that decade. While this is likely not how a modern president would use the word, it still feels topical given the issues nations were struggling to face at the time."
  },
  {
    "objectID": "textProject.html#sentiment-analysis",
    "href": "textProject.html#sentiment-analysis",
    "title": "Project 3",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nThere are a variety of methods/tools we have access to for evaluating the feelings expressed by text, but two big lexicons we will be using are AFINN and nrc.\nThe AFINN lexicon contains words with various emotional charges and for each, has a score ranging between -5 and 5 with negative scores indicating negative sentiment and positive scores indicating positive sentiment.\nThe nrc sorts words into categories os positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.\n\nafinn &lt;- get_sentiments(\"afinn\")\nnrc &lt;- get_sentiments(\"nrc\")\n\nWe can combine these lexicons into one large dataset, and peel off all the words with an emotional charge. Let’s also make a character vector where each entry is an inaugural address.\n\nnrc_full &lt;- nrc |&gt; \n  left_join(afinn) |&gt;\n  filter(!is.na(value))\n\nJoining with `by = join_by(word)`\n\nall_word &lt;- nrc_full$word\ninaug &lt;- as_tibble(data_corpus_inaugural)$value\n\nNow we have the tools ready to analyze our text. Let’s first look at all speeches overall and see what generalizations can be made for all addresses as a whole.\n\ncounts &lt;- c()\nindex &lt;- 1\nfor (i in all_word){\n  counts[index] &lt;- 0\n  \n  for(j in inaug){\n    if(str_detect(str_to_lower(j), paste0(\"\\\\b\", i, \"\\\\b\"))){\n#need to use paste0 function to add borders on other side of variable, otherwise words like \"assembly\" would get flagged for containing \"ass\" \n      counts[index] &lt;- counts[index] + 1\n    }\n  }\n  #print(counts[index])\n  index &lt;- index + 1\n  #print(index)\n  \n}\n\ncounts() is a vector containing the number of occurrences of each of the sentimental words we had in our record. Let’s add this into our dataset and calculate the total emotional value for all words over the course of every address.\n\nnrc_full &lt;- nrc_full |&gt;\n  mutate(n = counts, net_value = value*n)\n\n\ntopword &lt;- nrc_full |&gt;\n  arrange(desc(net_value)) |&gt;\n  distinct(word, n, net_value) |&gt;\n  slice_max(net_value, n = 20) |&gt;\n  mutate(positivity = \"good\")\n\n\nbottom_word &lt;- nrc_full |&gt;\n  arrange(net_value) |&gt;\n  distinct(word, n, net_value) |&gt;\n  slice_min(net_value, n = 20) |&gt;\n  mutate(positivity = \"bad\")\n\nextreme_word &lt;- bind_rows(topword, bottom_word)\n\n\nextreme_word |&gt;\n  mutate(word = fct_reorder(word, net_value)) |&gt;\n  ggplot(aes(word, net_value, fill = positivity)) +\n  geom_col() +\n  theme(axis.text.x = element_text(angle = 90, size = 10)) +\n  labs(x = \"Word\", y = \"Net Positivity\", title = \"Extreme Emotional Words for All Presidential Inaugural Addresses\", fill = \"Type\")\n\n\n\n\nAfter sorting, we see the word with the most profound positive effect when combining all our addresses is “good”, which may be an underwhelming choice, but it certainly makes sense that such a common word would make it to the top. Some more notable results have “hope”, “peace” and “justice” all tied for second place and “freedom” and “strength” not too far down from that. All of these highlight core, nationalistic values the US holds highly and provides a picture of what emotional impact the each president has wanted to convey as we usher in a new era of leadership.\nOn the other hand, these speeches can be a platform to address the threats looming over each presidential term and act as either a cautionary warning, or a denouncement of such beliefs.\nWe also can see that by far the most pervasive negative term in these addresses is “war” which is a surprisingly straightforward answer. The term has a very strong connection to the history of the US throughout all eras and is undeniably one of the largest challenges for the leadership of any nation. Another interesting thing to note is that slavery is 10th, and crime and debt are 13th and 14th. These fairly accurately reflect the most pressing issues the US has faced over the course of its entire history. However, although these negative keywords highlight the overall adversities experience by our nation, their presence may also serve to emphasize the resilience, determination, and resolve of Americans to overcome these obstacles.\nBecause of this, not all inaugural addresses are equal. Due to the historical contexts of the times, certain speeches may be more negative-sounding than others. The US has had 16 presidential addresses during times of war. Let’s highlight those 16 specifically, and see if they really do tend to have a more gloomy tone than the rest.\nFirst, we’ll make a new data set that has a total sentimental value attached to each presidential address.\n\nnewcounts &lt;- c()\nindex &lt;- 1\n\nfor(i in inaug){\n  newcounts[index] &lt;- 0\n  \n  for(j in all_word){\n    if(str_detect(str_to_lower(i), paste0(\"\\\\b\", j, \"\\\\b\"))){\n      newcounts[index] &lt;- newcounts[index] + nrc_full$value[which(nrc_full$word == j)[1]]\n    }\n  }\n  #print(newcounts[index])\n  index &lt;- index + 1\n  #print(index)\n}\n\n\npres_nrc &lt;- as_tibble(cbind(summary(data_corpus_inaugural)$Text, newcounts))\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\npres_nrc &lt;- pres_nrc |&gt;\n  mutate(newcounts = as.numeric(newcounts)) |&gt;\n  rename(address = \"V1\", positivity = \"newcounts\")\n\npres_nrc &lt;- pres_nrc |&gt;\n  left_join(summary(data_corpus_inaugural), by = join_by(address == Text))\n\n\npres_nrc |&gt;\nggplot(aes(address, positivity, fill = Party)) +\n  geom_col() +\n  theme(axis.text.x = element_text(angle = 90, size = 5)) +\n  labs(x = \"Address\", y = \"Positivity\", title = \"Overall Positivity of Presidential Inaugural Addresses\", caption = \"Based on Afinn scores\")\n\n\n\n\nPlotting this information shows that, in general, presidential inaugural addresses tend to be more positive than negative. It acts as a time for inspiring the nation and instilling hope for the future. Even when acknowledging uncertainty, the speeches still commit to addressing these challenges effectively. All this makes the few net negative speeches all the more interesting with what insight they can share for the era in which they were delivered.\nBy far the most negative was Lincoln’s first address given in 1861, just around a month before the US entered its deadliest war to date. We can also see two of FDR’s speeches also sink to the bottom as most negative, one for 1937 and another following in 1941, which coincides with WWII, the second deadliest war for the nation.\nOne more interesting part to observe is that President Garfield’s speech had an end total of 0 on the positivity scale. This is largely due to how his speech praised how proud he was of his nation, but condemned any acts he saw as a threat to it (namely the impediment of African-American sufferage)."
  },
  {
    "objectID": "mapProject.html",
    "href": "mapProject.html",
    "title": "Maps",
    "section": "",
    "text": "# Initial packages required (we'll be adding more)\nlibrary(tidyverse)\nlibrary(datasets)\nlibrary(fec16)\nlibrary(ggplot2)\nlibrary(tidycensus)\nlibrary(dplyr)\nlibrary(sf)\nlibrary(ggspatial)\nlibrary(mdsr)      # package associated with our MDSR book\n\n\nlibrary(maps)\nus_states &lt;- map_data(\"state\")\nhead(us_states)\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      &lt;NA&gt;\n2 -87.48493 30.37249     1     2 alabama      &lt;NA&gt;\n3 -87.52503 30.37249     1     3 alabama      &lt;NA&gt;\n4 -87.53076 30.33239     1     4 alabama      &lt;NA&gt;\n5 -87.57087 30.32665     1     5 alabama      &lt;NA&gt;\n6 -87.58806 30.32665     1     6 alabama      &lt;NA&gt;\n\n\n\nStatePopulation &lt;- read.csv(\"https://raw.githubusercontent.com/ds4stats/r-tutorials/master/intro-maps/data/StatePopulation.csv\", as.is = TRUE)\n\n\nus_rent_income |&gt;\n  filter(variable == \"income\") |&gt;\n  mutate(name = str_to_lower(NAME)) |&gt;\n  select(-NAME) |&gt;\n  right_join(us_states, by = c(\"name\" = \"region\")) |&gt;\n  ggplot(mapping = aes(x = long, y = lat,\n                          group = group)) + \n  geom_polygon(aes(fill = estimate), color = \"black\") +\n  labs(title = \"Average 2017 Income by State\", fill = \"Yearly Income (in USD)\") +\n  coord_map() +\n  theme_mdsr()\n\n\n\n\nWe can see that states like Maryland and Washington had a greater yearly income for 2017 while states like Alabama and West Virginia were not as fortuitous. It’s interesting how states like Maryland and Virginia are doing well while their neighbors are all on the other end of the spectrum. One other thing to note is that the Southeast US tended to earn less than most other regions.\n\n# summary of the 8 congressional Wisconsin districts and the 2016 voting\ndistrict_elections &lt;- results_house |&gt;\n  mutate(district = parse_number(district_id)) |&gt;\n  group_by(state, district) |&gt;\n  summarize(\n    N = n(), \n    total_votes = sum(general_votes, na.rm = TRUE),\n    d_votes = sum(ifelse(party == \"DEM\", general_votes, 0), na.rm = TRUE),\n    #we add together all the votes for democrats in the district\n    r_votes = sum(ifelse(party == \"REP\", general_votes, 0), na.rm = TRUE),\n    #same but with only republicans\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    other_votes = total_votes - d_votes - r_votes,\n    r_prop = r_votes / total_votes,  \n    winner = ifelse(r_votes &gt; d_votes, \"Republican\", \"Democrat\")\n  )\n\nwi_results &lt;- district_elections |&gt;\n  filter(state == \"WI\")\nwi_results |&gt;                  \n  select(-state)\n\n\n# Download congressional district shapefiles \noptions(timeout = 200)\nsrc &lt;- \"http://cdmaps.polisci.ucla.edu/shp/districts113.zip\"\nlcl_zip &lt;- fs::path(tempdir(), \"districts113.zip\")\ndownload.file(src, destfile = lcl_zip)\nlcl_districts &lt;- fs::path(tempdir(), \"districts113\")\nunzip(lcl_zip, exdir = lcl_districts)\ndsn_districts &lt;- fs::path(lcl_districts, \"districtShapes\")\n\n\n# read shapefiles into R as an sf object\nst_layers(dsn_districts)\n\n# be able to read as a data frame as well\ndistricts &lt;- st_read(dsn_districts, layer = \"districts113\") |&gt;\n  mutate(DISTRICT = parse_number(as.character(DISTRICT)))\nhead(districts, width = Inf)\nclass(districts)\n\n#####################################\n# create basic plot with Wisconsin congressional districts\nwi_shp &lt;- districts |&gt;\n  filter(STATENAME == \"Wisconsin\")\nwi_shp |&gt;\n  st_geometry() |&gt;\n  plot(col = gray.colors(nrow(wi_shp)))\n\nwi_merged &lt;- wi_shp |&gt;\n  st_transform(4326) |&gt;\n  inner_join(wi_results, by = c(\"DISTRICT\" = \"district\"))\nhead(wi_merged, width = Inf)\n\n\n# Color based on winning party\n#   Note that geom_sf is part of ggplot2 package, while st_geometry is\n#   part of sf package\nwi &lt;- ggplot(data = wi_merged, aes(fill = winner)) +\n  annotation_map_tile(zoom = 6, type = \"osm\", progress = \"none\") + \n  geom_sf(alpha = 0.5) +\n  scale_fill_manual(\"Winning Party\", values = c(\"blue\", \"red\")) + \n  geom_sf_label(aes(label = DISTRICT), fill = \"white\") + \n  theme_void() +\n  labs(title = \"Winners of Wisconsin's Eight Congressional Districts cir. 2016\")\nwi\n\n# Color based on proportion Rep.  Be sure to let limits so centered at 0.5.\n# This is a choropleth map, where meaningful shading relates to some attribute\nwi +\n  aes(fill = r_prop) + \n  scale_fill_distiller(\n    \"Proportion\\nRepublican\", \n    palette = \"RdBu\", \n    limits = c(0, 1)\n  )\n\nFrom this map, we can see that quite a few districts (like 2, 3, and 4) definitely contain a strong democratic population, while the districts that are more in the middle (like 1, 7 and 8) are just ever so slightly right leaning. For a party wanting to gerrymander, you would need to group all your opponent’s voters in a select few borders so that their votes go towards an overwhelming victory in those areas, while yours are spread around so that you use your total voters strategically, just barely pulling off a win. With this map, I can definitely understand the argument that Republicans have lopsided control of the state, since it seems like what few Democratic victories there are have very little Republican pushback."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cole Monson",
    "section": "",
    "text": "he/him/his\nMathematics + Computer Science Major with Statistics and Data Science Concentration at St. Olaf College \nSt. Olaf College\n\n\nA/V Engineer, Chapel Specialist | Broadcast Media\nTA/Grader | MSCS Department\nSt. Olaf Band (Tenor Sax), St. Olaf Handbells"
  },
  {
    "objectID": "index.html#cole-monson",
    "href": "index.html#cole-monson",
    "title": "Cole Monson",
    "section": "",
    "text": "he/him/his\nMathematics + Computer Science Major with Statistics and Data Science Concentration at St. Olaf College \nSt. Olaf College\n\n\nA/V Engineer, Chapel Specialist | Broadcast Media\nTA/Grader | MSCS Department\nSt. Olaf Band (Tenor Sax), St. Olaf Handbells"
  },
  {
    "objectID": "simulationProject.html",
    "href": "simulationProject.html",
    "title": "Simulation",
    "section": "",
    "text": "# Initial packages required\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(mdsr)"
  },
  {
    "objectID": "simulationProject.html#background",
    "href": "simulationProject.html#background",
    "title": "Simulation",
    "section": "Background",
    "text": "Background\nImagine we have new experimental technology to rid Minnesota of springtime snowstorms. Our team of researchers conducts an experiment using this new device over randomly selected areas of Northfield, and compares it with the other half that did not receive any treatment.\nLet’s say that this device really works and the weather has a significant decrease in snowfall for the affected areas. Despite this however, it might not always be clear to our researchers that our test had a significant effect. By chance, we could randomly collect a set of measurements from each group that doesn’t demonstrate a difference seen outside the realm of likelihood.\nIf we carried this test out a good number of times, we would hope to find a statistically low p-value a large proportion of the time. In other words, we would want our test’s power to be large enough so that we can reasonably expect our sample to provide data that correctly rejects the null hypothesis."
  }
]